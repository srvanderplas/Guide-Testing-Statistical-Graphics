---
title: Testing Statistical Graphics
author-list:
  - name: Emily Robinson
    dept: Statistics Department
    affiliation: CalPoly
    email: earobinson@huskers.unl.edu
  - name: Heike Hofmann
    dept: Statistics Department
    affiliation: Iowa State University
    email: hofmann@iastate.edu
  - name: Susan Vanderplas
    dept: Statistics Department
    affiliation: University of Nebraska, Lincoln
    email: svanderplas2@unl.edu

abstract: |
  We outline our process and common pitfalls for developing and testing human perception of statistical graphics. 
  Goal: Integrate information from the design of \cite{clusters,sineillusion,emily-diss,power}. 

category: Research
bib: refs.bib
bibstyle: class-info/abbrv-doi.bst
# csl: ieee-transactions-on-visualization-and-computer-graphics.csl
ieee-keywords: 
  - \CCScatTwelve{Human-centered computing}{Visu\-al\-iza\-tion}{Visualization design and evaluation methods}{}
  - \CCScatTwelve{Human-centered computing}{Visu\-al\-iza\-tion}{Empirical studies in visualization}{}
  - \CCScatTwelve{Human-centered computing}{Visu\-al\-iza\-tion}{Visualization application domains}{Scientific visualization}
  - \CCScatTwelve{Applied computing}{Physical sciences and engineering}{Mathematics and statistics}{}
firstsection: Introduction

link-citations: false

format:  
  pdf:
    keep-tex: true
    template-partials: [partials/doc-class.tex, partials/title.tex, partials/before-body.tex, partials/after-body.tex]

pdf-engine: pdflatex

date: last-modified
---

<!-- Importance of testing statistical graphics -->

Charts and data visualizations are primary methods of communicating scientific information to the public in what we typically hope is an easy-to-digest, visually attractive package.
There are many strategies for creating charts and graphs, from Tufte-esque minimalism\cite{tufte} to charts designed with extra imagery and aesthetic appeal that draw the viewer's attention and persist in memory\cite[Ch 3]{cairoFunctionalArtIntroduction2012}. 
For a specified type of data, there are also usually many different chart forms to display that data: for instance, if we have a set of categorical data and we wish to show the relative proportions of each category, we could do so using a stacked bar chart or the polar equivalent, a pie chart. 
There have been several attempts to list out all of the types of charts\cite{ribeccaSearchChartsData}, create a taxonomy of charts\cite{bertin1983semiology,desnoyersTaxonomyVisualsScience2011}, and even to classify charts using a domain-specific grammar of graphics\cite{wilkinsonGrammarGraphics1999}. 
With all of the different design choices available, how are chart creators to know what is the best approach for communicating data to the appropriate audience? 

There are heuristics, rules-of-thumb, and various guidelines \cite{asme-standards-graphics,allenVisualizingScientificData2016,haemerDoubleScalesAre1948,fewInformationDashboardDesign2006,kosslynGraphDesignEye2006} for creating useful and visually attractive charts, but as scientists, we believe empirical evidence is essential for making correct decisions. 
The best way to establish the efficacy of various design decisions, then, is to test our charts by having them evaluated by people under controlled conditions and gathering appropriate data\cite{clevelandShapeParameterTwoVariable1988,clevelandGraphicalPerceptionGraphical1985}. 
It can be extremely challenging to design studies which strike the right combination of internal and external validity. 
Simply asking people to read quantities off of a graph may not generalize beyond the questions asked or the data used in the chart\cite{eellsRelativeMeritsCircles1926,croxtonBarChartsCircle1927,vonhuhnFurtherStudiesGraphic1927,croxtonGraphicComparisonsBars1932}, but designing a study that is sufficiently robust to those issues requires manipulation of so many factors as to be functionally impossible to complete.
In this paper, we attempt to distill lessons learned from several different sets of experimental graphics experiments\cite{clusters,sineillusion,emily-diss,power}, discussing the use of several different testing methods for statistical graphics\cite{vanderplasTestingStatisticalCharts2020}, the process of designing a graphical experiment, and analysis of the resulting empirical data. 
It is our hope that by writing this paper, we will prevent other researchers from repeating hard lessons in what to do (and what not to do), lowering the barriers that exist for conducting research involving empirical tests of statistical graphics. 

# Testing Methods and Engagement
<!-- Lineups, You Draw It, Direct Assessment -->

We will start with an overview of different testing methods that we have used to empirically assess graphics. 
While it is certainly possible to use equipment such as eye-tracking devices, here we will primarily focus on studies which can be conducted over the internet, because these experiments have the lowest overhead, offer relatively fast data collection, and provide useful results that can be very clear if the experiment is designed correctly. 
Our toolkit is R-based\cite{R}, and includes ggplot2\cite{ggplot2} and Shiny\cite{shiny} as primary components, as well as  JavaScript (including D3\cite{d3}) to customize the Shiny interface for many of the experiments.

One primary concern when testing graphics is the level of user engagement required. 
For instance, if we are testing whether someone can detect an effect such as a trend line in noisy data, this is a *perceptual* question best addressed using methods which allow the user to interact with the data on a basic visual level. 
Sometimes, it may be possible to set up a scenario where the user adjusts the plot directly, as in our experiment testing the strength of the sine illusion experiment\cite{sineillusion}: users adjusted the strength of the illusion correction until the lines appeared to be the same length, providing a direct measure of the magnitude of the sine illusion's effect, as in @fig-sine-illusion. 

![Direct adjustment of a plot in a perceptual task](sine_illusion_screenshot.png){#fig-sine-illusion}

Statistical lineups\cite{bujaStatisticalInferenceExploratory2009,wickhamGraphicalInferenceInfovis2010} are a more generally applicable testing tool for perceptual questions such as "which chart displays this data more clearly"\cite{power}. 
As in the criminal procedure, lineups embed a target plot with signal into an array of 19 other innocent "null" plots that show only noise; if viewers consistently pick the target plot instead of any of the 19 null plots, then the target plot is said to be visually significant\cite{majumderValidationVisualStatistical2013,vanderplasStatisticalSignificanceCalculations2021} and a "see" value (the visual analogue of a p-value for a statistical test) can be calculated\cite{niladriroychowdhurySeeValueApp2020}.
In another variation of statistical lineup procedure, two models are compared, with target plots from each model embedded in the array of 20 total plots; the remaining null plots are constructed from a mixture model that blends the two competing models. This variation allows the experimenter to assess graphical design choices to determine whether they effectively emphasize structural differences in the data\cite{clusters}.
Statistical lineups do not require the experimenter to provide any contextual information for the data: all of the necessary information is embedded in the choice of a null statistical model, which is convenient for testing purposes, but does not require engagement at a level that would allow for testing the viewer's understanding of the information shown in the chart. 
An example of a statistical lineup for testing the perception of exponential growth is shown in @fig-lineup-log.

![A lineup experiment testing whether users can perceive differences in rates of exponential growth on linear and log scales. In addition to asking users to select the plot which is the most different, we also asked for basic reasoning and the user's level of confidence in their answer.](Lineup_log_screenshot.png){#fig-lineup-log}



To assess the viewer's *understanding* of information shown in a chart, we must ask questions and allow the user to provide feedback. User feedback may be collected on a numerical scale or through the use of written comments, recorded "think-aloud" processes, and other more qualitative interaction methods. 
We have conducted studies asking users to provide us with a written description of what the chart shows within a wider story-driven scenario (@fig-estimation-describe), and we have also asked for written feedback about why a certain panel of a statistical lineup was chosen; the insights from participant responses to these questions have been rich and insightful (and will be discussed more in the analysis section). 
While we have not (to date) recorded users talking out loud about what they are seeing, it is entirely feasible that so-called "think aloud" processes could be used within a Shiny application, with audio recordings saved to the server for transcription and analysis\cite{dunbar1995scientists,traftonTurningPicturesNumbers2000,kirschenbaum2003comparative}.

![This question asks users to write out a description of how the population of Ewoks changes over time, without any further cues, to determine whether participants default to multiplicative or additive descriptions.](Estimation_describe_plot_crop.png){#fig-estimation-describe}

Of course, as we have confined ourselves to the discussion of online, asynchronous testing methods, every user interaction with the testing materials (typically hosted on a web page) can also be recorded along with time stamps, mouse positions, and so on. 
While we have not used this information heavily in our experimental analyses thus far, we do collect some time stamp data in order to assess how long participants spend on each question; it could almost certainly be more informative in the right setting.
In addition, we have also provided participants with supportive tools, such as "scratch pads" and limited function calculators built into the Shiny application that allow us to gain additional insight into the estimation and visual comparison process when more complex calculations are required to answer the numerical estimation question (@fig-estimation-calc). 
These supportive tools have provided a wealth of information that, while not easy to analyze and generally messy, provides real insight into how participants were looking at the data, what strategies they tried and discarded for reading the chart, and what visual estimation methods were used. 

![This question asks participants for a numerical estimate, but provides a basic calculator and scratchpad. All user interactions with the calculator and scratchpad are logged, providing insight into the user's thought process.](Estimation_numerical_screenshot_crop.png){#fig-estimation-calc}

In our experience, one of the most difficult components of designing an experiment which asks users to directly estimate information from a chart is that the wording of such questions must be extremely carefully determined, and even when great care is taken with the construction of the question, participant answer accuracy is fundamentally limited by the fact that many participants do not read and interpret the question with the care and precision that it was written.
Questions that ask participants to e.g. estimate the multiplicative difference in a quantity at two timepoints may be misunderstood as asking for an estimate of the additive difference, and the resulting estimates are then one or more orders of magnitude off of the correct answer.
This is one area where lineup tests are definitely superior - they do not depend on participants to understand the nuances of language or scenarios built around the chart under investigation.


In some cases, we want participants to *engage directly* with an interactive visualization. 
We may want to record the way in which they interact with the chart to see what types of information they seek out, but we may also want to ask them to interact with the chart in a specific way and record the result. 
In one recent experiment, we asked participants to forecast an exponential trend, with data presented on either a linear or log scale. 
Using some JavaScript modified from New York Times interactive graphics "You Draw It" features\cite{katzYouDrawIt2017}, we had users draw trend lines and make forecasts directly on interactive charts, with the predictions and trend lines recorded to our database for later analysis^[See a gif of testing with You Draw It [here](https://i.imgur.com/GM5YSen.gif)].
With interactive graphics rendered using JavaScript (or other web libraries), the only limit to the types of questions you can ask in testing graphics is in your ability to write code to interact with the visualization library. 
This type of testing method can be extremely natural for participants, but it also is hard to generalize when discussing testing methods because of the potential range of applications where it might be employed.

Whichever testing method is chosen should be appropriate to the type of question under investigation. Lineups require a great amount of work in developing a null sampling model which replicates the important visual features of the data, but model specification is extremely important when asking users questions about any simulated data set or even when examining user interactions directly. It  is important to carefully calibrate the model parameters to allow for some variability, but not too much. This Goldilocks-style problem is the focus of the next section. 


# Developing a Model

Once you have decided what graphical task you want to explore or test, it is necessary to develop a model which can be used to test the graphical features you want to explore in a precise manner. 
This is the single longest part of the entire experimental design and execution process, in part because choosing a model that replicates important visual features of the data is an extremely complex process \cite{hullmanDesigningInteractiveExploratory2021,cookFoundationAvailableThinking2021,vanderplasDesigningGraphicsRequires2021}. 

There are two main options when developing a statistical model for graphical testing: you can either start with a large dataset and sample from that dataset\cite{power}, or you can start from a model and sample data from that model generating process\cite{sineillusion,clusters,emily-diss}.
This decision is largely determined by the availability of a large dataset containing the requisite features of interest and the qualities being manipulated in the experiment. For instance, Hofmann et al. \cite{power} used samples of different sizes from a pre-existing dataset to manipulate the amount of signal in each comparison; with a small sample, there is less signal and the same amount of noise, making the true plot harder to spot. 
In many situations, though, a convenient dataset with the right properties is harder to acquire, and it becomes necessary to develop a sampling model to generate data for user evaluation. 

The tools we discuss in the remainder of this section can be applied both to pre-existing data sets and to model-based sampling methods. 
We will do our best to keep this discussion general, but we will provide details in some situations to ensure that our general descriptions are not hopelessly vague. 


## Screening Parameters with Simulation

Visual assessment is key, but narrowing the parameter space down to something that can be statistically determined is useful for providing a sort of numerical magnitude of difficulty assessment. 
If using model based sampling methods, it is important to iteratively assess the sampling procedure through simulation: Start with a sampling method and define a parameter space that is appropriate for the model, and then simulate many data sets from each possible parameter combinations using a grid search.
When sampling from a large data set, it is still important to assess the sample size and any potential stratification methods employed in order to ensure that the visual effect is represented.

In both cases, it can help to employ some general feature engineering - what numerical statistic provides an estimation of the basic concept which is being visually evaluated? For instance, we have used

- $R^2$ as a measure of the strength of a linear relationship
- Gini inequality as a measure of the strength of clustering
- lack-of-fit statistics to assess the amount of curvature in an exponential relationship (shown in @fig-lof-density-curves)

Then, a wide range of potential combinations of parameter values or sampling strategies can be explored and summarized graphically; if the numerical statistic cannot differentiate between the null and target under a condition, it is reasonable to expect that a visual inspection of the data may also not show significant results.
As with any measure, it is important that difficulty levels span a range from easy to hard; we do not learn anything from finding out that everyone can distinguish all of the combinations.

```{r lof-density-curves, echo = F, include = T, message = F, warning = F}
#| label: fig-lof-density-curves
#| fig-cap: "Density plot of the lack of fit statistic showing separation of selected difficulty levels: High (obvious curvature), Medium (noticable curvature), and Low (almost linear). Each density plot is the result of 1000 simulations from a model $y_i = \\alpha\\cdot e^{\\beta\\cdot x_i + \\epsilon_i} + \\theta$, where $\\epsilon \\sim N(0, \\sigma^2)$. $\\alpha$ and $\\theta$ were selected after manipulation of $\\beta$ and $\\sigma$ to ensure that all data generated had similar $y$ ranges so as not to provide visual cues about model differences outside of the plot curvature."
#| out.width: "\\columnwidth"
#| fig.height: 2.5
#| fig.width: 5

library(tidyverse)
lofData <- read.csv(file = "https://raw.githubusercontent.com/earobinson95/EmilyARobinson-UNL-dissertation/main/data/01-lineups/lineup-lof-data.csv")
lofPlot_curvature <- lofData %>%
  mutate(Curvature = factor(Curvature, levels = c("Obvious Curvature", "Noticeable Curvature", "Almost Linear"), labels = c("High Curvature", "Medium Curvature", "Low Curvature"))) %>%
  mutate(Variability = factor(Variability, levels = c("Low"))) %>%
  ggplot(aes(x = statistic, fill = Curvature, color = Curvature)) +
  geom_density(alpha = 0.7) +
  scale_fill_manual("Difficulty", values = c("#004400", "#116611", "#55aa55")) +
  scale_color_manual("Difficulty", values = c("#004400", "#116611", "#55aa55")) +
  theme_bw(base_size = 14) +
  theme(legend.position = "bottom",
        axis.text    = element_text(size = 10),
        axis.title   = element_text(size = 10),
        legend.title = element_text(size = 10),
        legend.text  = element_text(size = 10),
        legend.key.size = unit(0.5, "line")
        ) + 
  scale_x_continuous("Lack of Fit Statistic") +
  scale_y_continuous("Density") + 
  theme(legend.position = c(1, 1), legend.justification = c(1.01, 1.01), legend.box.background = element_rect(fill = "white", color = "black"))
lofPlot_curvature
```

## Fine-Tuning Parameter Choices

Once an appropriate set of parameters are identified using the numerical screening method, then it is important to calibrate these parameter selections visually; after all, no numerical statistic is a perfect measure of what we actually see; at best, they are approximations of what we might potentially see. 
We have found it to be useful to have one experimenter calibrate the model parameters at a gross level, and then have another experimenter narrow in on the parameters which are visually reasonable within the selected range. Then, both examiners visually inspect a large number of plots generated using those parameters to get a sense for how difficult the task at hand is (this strategy is also described by Lu et al. \cite{luModelingJustNoticeable2022}). 
At some point, all experimenters become so visually saturated with the nuances of the data generating mechanism that it may become necessary to "sanity check" the protocol with family members, friends, and colleagues. 
These informal surveys provide extremely useful feedback and can help to counteract the visual saturation of being immersed in the design of a visualization experiment for months at a time. 


## Visual Assessment is Critical

We cannot overstate the importance of visual assessment of your model stimuli, preferrably with fresh eyes. We highly recommend performing a series of think-aloud pilot testing before deploying an experiment. In support of this assessment, we offer up a cautionary tale of our own experience: that of \cite{clusters}, where we designed an experiment to test which plot aesthetics promoted discovery of linear trends and/or clusters. 

The experiment was a full 2x3x3 factorial exploration of three data generating parameters, with 3 replicates at each parameter combination (54 datasets) and 10 aesthetic combinations (for a total of 540 lineups). Each lineup had 20 different sub-panels, so we should have carefully visually inspected some 10800 different panels. 
As is evident from the fact that we're telling this story as a cautionary tale, we missed a critical problem with our data-generating mechanism: when clusters were assigned to randomly generated data after the fact, we didn't control the cluster size, leading to clusters of one or two points in relatively few sub-panels. 
This became particularly noticeable when bounding ellipses were added to the plot, as the method used to generate those ellipses required at least 3 points in the cluster. 
Many sub-plots were generated with only 2 or 4 ellipses (with 3 or 5 total clusters), which escaped our notice during the stimuli proof-reading phase of the experiment, but did not escape the notice of our participants, who only needed to examine about 10 lineups each (around 200 panels). 
An example of one of the problematic lineups is shown in @fig-lineup-problems: many participants selected panel 16 because of the missing ellipse; not a wrong choice, but certainly not the visual effect we intended to test. 

![A lineup from Vanderplas & Hofmann \cite{clusters}. Panel 10 shows the clustered target data and panel 17 shows the target data with a strong linear relationship; either of these target panels was the expected choice. Unfortunately, panel 16 has only two bounding ellipses shown, which is an unintentional difference that resulted from a faulty method for assigning clusters to null plots; many participants selected this panel instead of one of the target panels.](lineup-missing-ellipse.png){#fig-lineup-problems}

One reason why it is so difficult to generate sampling models for visual explorations is that our visual systems are very, very good at finding differences and unexpected results. 
We re-ran the experiment using a different clustering method, and found that instead of cuing in on the number of ellipses, when that variable was removed participants instead cued in on the differences in size and shape of the ellipses formed using k-means clustering after the data generating procedure. 
That is, participants could still detect the artificial nature of the induced clusters using other features. 
While it can be difficult to get the data generating method right, it is essential to conducting visual experiments that generalize well beyond the effects shown in a single data set or phenomenon. 
The time and effort invested in this step at the outset of the experiment pays dividends when it allows for clear generalization of the experimental results to an entire statistical concept rather than a single data set.  
 

# Experimental Design Considerations 

- Attention span
- Sensory overload - too many lineups in a row is hard
- Comparing across participants - importance of random effects for participant skill level
- Consider asking both demographic questions and questions about mathematical skill level. We would have been wise to ask about country of primary/secondary education in our log study, as logarithms are introduced at different levels in different countries. But we didn't think of that when recruiting participants, so it would have been hard to manipulate that factor. There are always missed opportunities.

- We've had lots of success with complete and incomplete block designs

- Importance of randomizing trial order to combat order effects

# Pilot Testing and Quality Assurance

Test early, test often, and make sure you look at all of your plots!!!
If you're generating new data for each participant, be sure to save that data to a database so that you can at least see where/how things went wrong!

# Data Collection and Monitoring

Prolific, Amazon Turk, testing with interested subgroups.

Use of pilot data to set up analysis documents, github actions and/or shiny dashboard to monitor incoming results.

Not necessarily important for small tests of limited duration, because data from e.g. prolific comes in so quickly. Server load is actually a much greater concern. 


# Analyzing the Data
Be prepared for surprises

Use of linear models to test lineup data - see-value approach is great for single plots, but when we're interested in more concrete effects over many plots, it can be better to aggregate effects and just think of a multinomial model.

Direct estimation can be tricky to analyze because of how messy the data is. Additional supplemental information can be critical to provide context, but graphical analysis of participant responses was more helpful than modeling for our experiment. 

Useful graphical analysis methods: density plots + rug plots (so that you can see rounding effects and common errors), wordclouds, estimation sketches + spaghetti plots

## Analyzing Supplemental Information

Calculator observations
Scratchpad
Explanation
Confidence Level
User Interactions

# Conclusion

\clearpage